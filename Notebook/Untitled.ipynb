{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.ops import clip_ops\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def open_data(direc,ratio_train = 0.8,dataset = \"ECG5000\"):\n",
    "  \"\"\"Input:\n",
    "  direc: location of the UCR archive\n",
    "  ratio_train: ratio to split training and testset\n",
    "  dataset: name of the dataset in the UCR archive\"\"\"\n",
    "  datadir = direc + '/' + dataset + '/' + dataset\n",
    "  data_train = np.loadtxt(datadir+'_TRAIN',delimiter=',')\n",
    "  data_test_val = np.loadtxt(datadir+'_TEST',delimiter=',')[:-1]\n",
    "  data = np.concatenate((data_train,data_test_val),axis=0)\n",
    "\n",
    "  N,D = data.shape\n",
    "\n",
    "  ind_cut = int(ratio_train*N)\n",
    "  ind = np.random.permutation(N)\n",
    "  return data[ind[:ind_cut],1:],data[ind[ind_cut:],1:],data[ind[:ind_cut],0],data[ind[ind_cut:],0]\n",
    "\n",
    "\"\"\"Plot the data\"\"\"\n",
    "def plot_data(X_train, y_train, plot_row = 5):\n",
    "  counts = dict(Counter(y_train))\n",
    "  num_classes = len(np.unique(y_train))\n",
    "  f, axarr = plt.subplots(plot_row, num_classes)\n",
    "  for c in np.unique(y_train):    #Loops over classes, plot as columns\n",
    "    c = int(c)\n",
    "    ind = np.where(y_train == c)\n",
    "    ind_plot = np.random.choice(ind[0],size=plot_row)\n",
    "    for n in xrange(plot_row):  #Loops over rows\n",
    "      axarr[n,c].plot(X_train[ind_plot[n],:])\n",
    "      # Only shops axes for bottom row and left column\n",
    "      if n == 0: axarr[n,c].set_title('Class %.0f (%.0f)'%(c,counts[float(c)]))\n",
    "      if not n == plot_row-1:\n",
    "        plt.setp([axarr[n,c].get_xticklabels()], visible=False)\n",
    "      if not c == 0:\n",
    "        plt.setp([axarr[n,c].get_yticklabels()], visible=False)\n",
    "  f.subplots_adjust(hspace=0)  #No horizontal space between subplots\n",
    "  f.subplots_adjust(wspace=0)  #No vertical space between subplots\n",
    "  plt.show()\n",
    "  return\n",
    "  \n",
    "def plot_z_run(z_run,label):\n",
    "  from sklearn.decomposition import TruncatedSVD\n",
    "  f1, ax1 = plt.subplots(2, 1)\n",
    "\n",
    "  PCA_model = TruncatedSVD(n_components=3).fit(z_run)\n",
    "  z_run_reduced = PCA_model.transform(z_run)\n",
    "  ax1[0].scatter(z_run_reduced[:,0],z_run_reduced[:,1],c=label,marker='*',linewidths = 0)\n",
    "  ax1[0].set_title('PCA on z_run')\n",
    "#  ax1[0,1].scatter(z_run_reduced[:,2],z_run_reduced[:,1],c=label,marker='*',linewidths = 0)\n",
    "#  ax1[0,0].set_title('PCA on z_run')\n",
    "#  ax1[1,1].scatter(z_run_reduced[:,2],z_run_reduced[:,0],c=label,marker='*',linewidths = 0)\n",
    "#  ax1[0,1].set_title('PCA on z_run')\n",
    "  \n",
    "  from sklearn.manifold import TSNE\n",
    "  tSNE_model = TSNE(verbose=2, perplexity=30,min_grad_norm=1E-12,n_iter=3000)\n",
    "  z_run_tsne = tSNE_model.fit_transform(z_run)\n",
    "  ax1[1].scatter(z_run_tsne[:,0],z_run_tsne[:,1],c=label,marker='*',linewidths = 0)\n",
    "  ax1[1].set_title('tSNE on z_run')\n",
    "  return\n",
    "\n",
    "class Model():\n",
    "  def __init__(self,config):\n",
    "    \"\"\"Hyperparameters\"\"\"\n",
    "    num_layers = config['num_layers']\n",
    "    hidden_size = config['hidden_size']\n",
    "    max_grad_norm = config['max_grad_norm']\n",
    "    batch_size = config['batch_size']\n",
    "    sl = config['sl']\n",
    "    crd = config['crd']\n",
    "    num_l = config['num_l']\n",
    "    learning_rate = config['learning_rate']\n",
    "    self.sl = sl\n",
    "    self.batch_size = batch_size\n",
    "\n",
    "\n",
    "    # Nodes for the input variables\n",
    "    self.x = tf.placeholder(\"float\", shape=[batch_size, sl], name = 'Input_data')\n",
    "    self.x_exp = tf.expand_dims(self.x,1)\n",
    "    self.keep_prob = tf.placeholder(\"float\")\n",
    "\n",
    "    with tf.variable_scope(\"Encoder\") as scope:  \n",
    "      #Th encoder cell, multi-layered with dropout\n",
    "      cell_enc = tf.nn.rnn_cell.LSTMCell(hidden_size)\n",
    "      cell_enc = tf.nn.rnn_cell.MultiRNNCell([cell_enc] * num_layers)\n",
    "      cell_enc = tf.nn.rnn_cell.DropoutWrapper(cell_enc,output_keep_prob=self.keep_prob)\n",
    "\n",
    "      #Initial state\n",
    "      initial_state_enc = cell_enc.zero_state(batch_size, tf.float32)\n",
    "\n",
    "      outputs_enc,_ = tf.nn.seq2seq.rnn_decoder(tf.unpack(self.x_exp,axis=2),initial_state_enc,cell_enc)\n",
    "      cell_output = outputs_enc[-1]  #Only use the final hidden state #tensor in [batch_size,hidden_size]\n",
    "    with tf.name_scope(\"Enc_2_lat\") as scope:\n",
    "      #layer for mean of z\n",
    "      W_mu = tf.get_variable('W_mu', [hidden_size,num_l])\n",
    "      b_mu = tf.get_variable('b_mu',[num_l])\n",
    "      self.z_mu = tf.nn.xw_plus_b(cell_output,W_mu,b_mu,name='z_mu')  #mu, mean, of latent space\n",
    "      \n",
    "      #Train the point in latent space to have zero-mean and unit-variance on batch basis\n",
    "      lat_mean,lat_var = tf.nn.moments(self.z_mu,axes=[1])\n",
    "      self.loss_lat_batch = tf.reduce_mean(tf.square(lat_mean)+lat_var - tf.log(lat_var)-1)\n",
    "      \n",
    "    with tf.name_scope(\"Lat_2_dec\") as scope:\n",
    "      #layer to generate initial state\n",
    "      W_state = tf.get_variable('W_state', [num_l,hidden_size])\n",
    "      b_state = tf.get_variable('b_state',[hidden_size])\n",
    "      z_state = tf.nn.xw_plus_b(self.z_mu,W_state,b_state,name='z_state')  #mu, mean, of latent space\n",
    "      \n",
    "    with tf.variable_scope(\"Decoder\") as scope:\n",
    "      # The decoder, also multi-layered\n",
    "      cell_dec = tf.nn.rnn_cell.LSTMCell(hidden_size)\n",
    "      cell_dec = tf.nn.rnn_cell.MultiRNNCell([cell_dec] * num_layers)\n",
    "\n",
    "      #Initial state\n",
    "      initial_state_dec = tuple([(z_state,z_state)]*num_layers)\n",
    "      dec_inputs = [tf.zeros([batch_size,1])]*sl\n",
    "      outputs_dec,_ = tf.nn.seq2seq.rnn_decoder(dec_inputs,initial_state_dec,cell_dec)\n",
    "    with tf.name_scope(\"Out_layer\") as scope:\n",
    "      params_o = 2*crd   #Number of coordinates + variances\n",
    "      W_o = tf.get_variable('W_o',[hidden_size,params_o])\n",
    "      b_o = tf.get_variable('b_o',[params_o])\n",
    "      outputs = tf.concat(0,outputs_dec)                    #tensor in [sl*batch_size,hidden_size]\n",
    "      h_out = tf.nn.xw_plus_b(outputs,W_o,b_o)\n",
    "      h_mu,h_sigma_log = tf.unstack(tf.reshape(h_out,[sl,batch_size,params_o]),axis=2)\n",
    "      h_sigma = tf.exp(h_sigma_log)\n",
    "      dist = tf.contrib.distributions.Normal(h_mu,h_sigma)\n",
    "      px = dist.pdf(tf.transpose(self.x))\n",
    "      loss_seq = -tf.log(tf.maximum(px, 1e-20))             #add epsilon to prevent log(0)\n",
    "      self.loss_seq = tf.reduce_mean(loss_seq)\n",
    "      \n",
    "    with tf.name_scope(\"train\") as scope:\n",
    "      #Use learning rte decay\n",
    "      global_step = tf.Variable(0,trainable=False)\n",
    "      lr = tf.train.exponential_decay(learning_rate,global_step,1000,0.1,staircase=False)\n",
    "      \n",
    "      \n",
    "      self.loss = self.loss_seq + self.loss_lat_batch\n",
    "      \n",
    "      #Route the gradients so that we can plot them on Tensorboard\n",
    "      tvars = tf.trainable_variables()\n",
    "      #We clip the gradients to prevent explosion\n",
    "      grads = tf.gradients(self.loss, tvars)\n",
    "      grads, _ = tf.clip_by_global_norm(grads,max_grad_norm)\n",
    "      self.numel = tf.constant([[0]])\n",
    "\n",
    "      #And apply the gradients\n",
    "      optimizer = tf.train.AdamOptimizer(lr)\n",
    "      gradients = zip(grads, tvars)\n",
    "      self.train_step = optimizer.apply_gradients(gradients,global_step=global_step)\n",
    "#      for gradient, variable in gradients:  #plot the gradient of each trainable variable\n",
    "#        if isinstance(gradient, ops.IndexedSlices):\n",
    "#          grad_values = gradient.values\n",
    "#        else:\n",
    "#          grad_values = gradient\n",
    "#\n",
    "#        self.numel +=tf.reduce_sum(tf.size(variable))\n",
    "#        tf.summary.histogram(variable.name, variable)\n",
    "#        tf.summary.histogram(variable.name + \"/gradients\", grad_values)\n",
    "#        tf.summary.histogram(variable.name + \"/gradient_norm\", clip_ops.global_norm([grad_values]))\n",
    "\n",
    "      self.numel = tf.constant([[0]])\n",
    "    tf.summary.tensor_summary('lat_state',self.z_mu)\n",
    "    #Define one op to call all summaries\n",
    "    self.merged = tf.summary.merge_all()\n",
    "    #and one op to initialize the variables\n",
    "    self.init_op = tf.global_variables_initializer()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
